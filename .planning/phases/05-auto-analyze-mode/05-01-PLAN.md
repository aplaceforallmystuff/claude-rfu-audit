---
phase: 05-auto-analyze-mode
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - skill/guides/AUTO-ANALYZE.md
autonomous: true

must_haves:
  truths:
    - "Auditor knows exactly what fields auto-analyze extracts"
    - "Auditor understands confidence levels (HIGH/MEDIUM/LOW/UNCERTAIN)"
    - "Human verification is required before using extracted data"
    - "Extraction failures degrade gracefully to manual entry"
  artifacts:
    - path: "skill/guides/AUTO-ANALYZE.md"
      provides: "Extraction heuristics and verification workflow guide"
      min_lines: 200
      contains: "Extraction Prompt Template"
  key_links:
    - from: "skill/guides/AUTO-ANALYZE.md"
      to: "config/gates-full.md"
      via: "field-to-gate mapping"
      pattern: "Gate [0-9]+"
---

<objective>
Create AUTO-ANALYZE.md guide documenting extraction heuristics, confidence scoring, human verification workflow, and graceful degradation patterns.

Purpose: Auditors need a reference for how auto-analyze extracts data from project files, what confidence levels mean, and how to verify suggestions before scoring gates.

Output: `skill/guides/AUTO-ANALYZE.md` â€” complete guide for the auto-analyze feature
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-auto-analyze-mode/05-RESEARCH.md

# Reference for guide format
@skill/guides/INPUT-VALIDATION.md
@skill/guides/GATE-EXAMPLES.md

# Reference for gate details (extraction maps to these)
@skill/config/gates-full.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AUTO-ANALYZE.md with extraction template and field definitions</name>
  <files>skill/guides/AUTO-ANALYZE.md</files>
  <action>
Create `skill/guides/AUTO-ANALYZE.md` with the following structure:

**1. Overview section:**
- What auto-analyze does (hypothesis generation, not fact-finding)
- Core principle: "Claude suggests, human verifies"
- When to use (first-time audit, unfamiliar project)

**2. Fields Extracted section:**
Create table mapping each extracted field to its gate usage:

| Field | Gate(s) | Source Priority | Confidence Heuristic |
|-------|---------|-----------------|----------------------|
| project_name | Report header | package.json name > README H1 | HIGH if explicit |
| one_line_description | Gate 4 (Bartender) | README first paragraph > package.json description | HIGH if single sentence, MEDIUM if extracted from longer text |
| value_proposition | Gates 1, 2 | "Why/Problem/Motivation" sections > first paragraph after H1 | MEDIUM (usually inferred) |
| target_users | Gates 1, 5 | "Who is this for/Audience" sections > usage examples | MEDIUM to LOW |
| key_features | Gate 10 (Pareto) | "Features" section > bullet lists in top 30% | HIGH if section exists |
| installation_time_estimate | Gate 3 (10-Minute) | Prerequisites count + steps count | MEDIUM (always estimate) |
| prerequisites | Gate 3 | "Prerequisites/Requirements" section | HIGH if section exists |

**3. Extraction Prompt Template section:**
Include the full structured prompt from research (lines 470-532 of 05-RESEARCH.md) with:
- README and package.json placeholders
- JSON output format with all fields
- Confidence levels per field
- Source attribution requirements
- Extraction rules (grounding constraints)

**4. Confidence Levels section:**
Define each level with criteria:
- HIGH: Explicit in docs, unambiguous, single source
- MEDIUM: Inferred from context, multiple sources agree
- LOW: Guessed, conflicting sources, minimal info
- UNCERTAIN: Not present, too ambiguous to extract

Include visual indicators: checkmark (HIGH), warning (MEDIUM), question (LOW/UNCERTAIN)

**5. README Section Identification Heuristics section:**
Document where to find each field type (from research lines 184-209):
- One-line description: First sentence after H1, text between title and first H2
- Value proposition: "Why/Problem/Motivation" sections, imperative patterns ("Stop...", "Never...")
- Features: "Features/Capabilities" sections, bullet lists with action verbs
- Installation time: Step count in Installation section, prerequisites complexity
- Target users: "Who is this for" sections, "for X who" phrases

**6. Graceful Degradation section:**
Document what happens when extraction fails:
- UNCERTAIN fields prompt manual entry
- Partial extraction still proceeds (only project_name truly required)
- Error message format: "Could not extract [field]: [reason]. Please enter manually."
- Options: Enter now, defer to gate scoring, mark N/A
  </action>
  <verify>
File exists at `skill/guides/AUTO-ANALYZE.md` with:
- All 7 fields documented in extraction table
- Complete extraction prompt template with JSON format
- All 4 confidence levels defined with criteria
- README section heuristics for each field type
- Graceful degradation patterns documented
  </verify>
  <done>
AUTO-ANALYZE.md exists with extraction heuristics covering all fields, confidence scoring rules, and degradation patterns
  </done>
</task>

<task type="auto">
  <name>Task 2: Add human verification workflow and conflict handling</name>
  <files>skill/guides/AUTO-ANALYZE.md</files>
  <action>
Add to AUTO-ANALYZE.md after the Graceful Degradation section:

**7. Human Verification Workflow section:**
Document the verification interaction pattern:

```
## Auto-Analyze Results

Claude extracted the following from your project files.
Review each field and approve, edit, or reject.

### {field_name}
**Extracted:** {value}
**Source:** {source_attribution}
**Confidence:** {level} {icon}

**Actions:**
[1] Approve as-is
[2] Edit value
[3] Reject and enter manually
[4] Show source in files
```

Include rules:
- HIGH confidence: Default to approved, quick Enter to accept
- MEDIUM confidence: Requires explicit review before proceeding
- LOW confidence: Default to rejected, suggest manual entry
- UNCERTAIN: No suggestion provided, prompt manual entry

Batch actions:
- 'a' = approve all HIGH confidence
- 'r' = review MEDIUM/LOW fields
- 'q' = quit auto-analyze, full manual entry

**8. Conflict Handling section:**
Document what to do when README and package.json disagree:
- Flag as MEDIUM confidence (conflict detected)
- Show both values: "README says X, package.json says Y"
- Suggest README version (more user-facing)
- Offer options: Approve suggested, use package.json, write custom

**9. Pitfalls to Avoid section:**
Document common mistakes (from research lines 276-461):
- Automation bias: Rubber-stamping without verification
- Missing info as failure: Should degrade to manual, not error
- Conflicting info not flagged: Must surface conflicts
- Over-extraction: Hallucinating features not in files
- Installation time misestimation: Use conservative estimates

**10. Limitations section:**
Explicit statement of what auto-analyze cannot do:
- Cannot score gates (extraction only, not judgment)
- Cannot guarantee accuracy (suggestions, not facts)
- Cannot replace human verification (required step)
- Works best with standard README structure (may fail on unconventional docs)
  </action>
  <verify>
AUTO-ANALYZE.md includes:
- Human verification workflow with interaction format
- Confidence-based default states (HIGH=approved, LOW=rejected)
- Batch action documentation (a/r/q)
- Conflict handling with both-values display
- At least 4 pitfalls documented
- Limitations section with explicit constraints
  </verify>
  <done>
AUTO-ANALYZE.md is complete with verification workflow, conflict handling, pitfalls, and limitations documented
  </done>
</task>

</tasks>

<verification>
1. File exists: `skill/guides/AUTO-ANALYZE.md`
2. Structure check: Contains all 10 major sections (Overview through Limitations)
3. Field coverage: All 7 extraction fields documented with gate mappings
4. Confidence levels: All 4 levels defined with clear criteria
5. Workflow documented: Human verification interaction pattern included
6. Graceful degradation: Failure handling documented for each field type
7. Cross-references: Links to config/gates-full.md for gate details
</verification>

<success_criteria>
- AUTO-ANALYZE.md is >200 lines
- All extraction fields map to specific gates
- Confidence scoring criteria are unambiguous
- Human verification is documented as required (not optional)
- Graceful degradation ensures extraction failure does not block audit
- Guide follows same structural patterns as INPUT-VALIDATION.md
</success_criteria>

<output>
After completion, create `.planning/phases/05-auto-analyze-mode/05-01-SUMMARY.md`
</output>
