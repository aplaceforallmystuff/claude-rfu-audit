---
phase: 02-gate-clarity
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - skill/config/gates-full.md
  - skill/guides/GATE-EXAMPLES.md
autonomous: true

must_haves:
  truths:
    - "Gates 1-4 have explicit pass/fail thresholds an auditor can apply without interpretation"
    - "Each of Gates 1-4 has 2+ FAIL examples showing clear failure patterns"
    - "Auditor can score Gates 1-4 identically to another auditor given same project"
  artifacts:
    - path: "skill/config/gates-full.md"
      provides: "Objective rubrics for Gates 1-4"
      contains: "## Objective Rubric"
    - path: "skill/guides/GATE-EXAMPLES.md"
      provides: "FAIL examples for Gates 1-4"
      contains: "### FAIL Example"
  key_links:
    - from: "skill/config/gates-full.md"
      to: "skill/guides/GATE-EXAMPLES.md"
      via: "Gate rubrics reference examples"
      pattern: "See.*GATE-EXAMPLES.md"
---

<objective>
Add objective scoring rubrics and FAIL examples for Gates 1-4 (5 Whys, Inversion Test, 10-Minute Test, Bartender Test).

Purpose: Enable consistent scoring by replacing vague criteria with countable/measurable thresholds and providing concrete failure examples that define the floor.

Output: Updated gates-full.md with rubric sections for Gates 1-4, updated GATE-EXAMPLES.md with 2+ FAIL examples per gate.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-gate-clarity/02-RESEARCH.md

# Source files to modify
@skill/config/gates-full.md
@skill/guides/GATE-EXAMPLES.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add objective rubrics to Gates 1-4 in gates-full.md</name>
  <files>skill/config/gates-full.md</files>
  <action>
For each gate (1, 2, 3, 4), add three new sections after the existing "Fail indicators" section:

**## Objective Rubric**
Contains specific measurement protocol with thresholds. Use patterns from research:

**Gate 1 (5 Whys):**
- Pass: Chain reaches fundamental human need (autonomy/time/money/status/connection/safety) within 5 iterations
- Borderline: Chain reaches 5 iterations without clear bedrock, but each why builds logically on previous
- Fail: Chain stops at technical reason, loops, or cannot proceed past 3 iterations
- Disqualifier: Any why answered with "because I wanted to learn" or "because it's cool" without deeper exploration

**Gate 2 (Inversion Test):**
- Measure switching cost to alternatives:
  - Alternative exists + ≤5 min to switch + no data loss → FAIL (convenience only)
  - Alternative exists + 1-4 hours to switch → BORDERLINE (apply quantitative test: >20% time/cost savings vs alternative = PASS)
  - Alternative exists + >4 hours to switch OR data loss OR recurring cost → PASS
  - No alternative exists → PASS
- Disqualifier: "Minor inconvenience" without specific workflow impact

**Gate 3 (10-Minute Test):**
- Measure total time: discovery + comprehension + install + first use + aha moment
- Pass: Total ≤10 minutes with real value demonstrated
- Borderline: 10-15 minutes (apply test: Would user continue past 10 min?)
- Fail: >15 minutes OR no clear aha moment within any time frame
- Pre-requisite rule: Include pre-req time if project-specific or uncommon (<30% of target users have it); exclude if domain-standard (Node for JS, Python for Python)

**Gate 4 (Bartender Test):**
- Binary checklist:
  - [ ] Zero technical jargon (API, CLI, schema, deploy are jargon; email, file, website are not)
  - [ ] Describes value/outcome, not implementation/features
  - [ ] Single sentence (compound clause OK, two independent sentences FAIL)
  - [ ] No follow-up questions needed for comprehension
- Pass: All 4 checks = Yes
- Fail: Any check = No
- Grandmother test tie-breaker: Would your grandmother understand this with no context?

**## Scoring**
Explicit conditions for each verdict (Pass/Borderline/Fail).

**## Edge Cases**
Brief pointers to common gray areas (will be expanded in guides/EDGE-CASES.md later):
- Gate 1: "Learning motivation" - see EDGE-CASES.md
- Gate 2: "Minor inconvenience boundary" - see EDGE-CASES.md
- Gate 3: "Pre-requisite timing" - see EDGE-CASES.md
- Gate 4: "Domain-specific terms" - see EDGE-CASES.md

Keep existing content (Process, Pass criteria, Fail indicators, Template Variables, See Also). Add new sections AFTER "Fail indicators" and BEFORE "Template Variables".
  </action>
  <verify>
grep -c "## Objective Rubric" skill/config/gates-full.md should return 4 (one per gate)
grep -c "## Scoring" skill/config/gates-full.md should return 4
grep -c "## Edge Cases" skill/config/gates-full.md should return 4
  </verify>
  <done>Gates 1-4 each have Objective Rubric, Scoring, and Edge Cases sections with measurable thresholds</done>
</task>

<task type="auto">
  <name>Task 2: Add FAIL examples for Gates 1-4 to GATE-EXAMPLES.md</name>
  <files>skill/guides/GATE-EXAMPLES.md</files>
  <action>
Replace the placeholder content for Gates 1-4 with concrete FAIL examples. Follow the structure from research:

For each gate, provide:
1. **FAIL Example 1** (clear failure)
2. **FAIL Example 2** (borderline that should fail)
3. Optional: **PASS Example** for contrast

Structure each FAIL example as:
```markdown
### FAIL Example 1: [Descriptive Name]

**Project type:** [e.g., CLI tool, API wrapper, automation script]

**Scenario:** [Brief project description]

**Attempted answer/evidence:**
[What the auditor found]

**Why this FAILS:**
1. [Specific reason tied to rubric]
2. [Another specific reason]
3. [Pattern this represents]

**Contrast with PASS:** [Brief description of what passing would look like]
```

Example FAIL scenarios to include:

**Gate 1 (5 Whys) FAIL Examples:**
1. "API wrapper that stops at 'because it makes API calls easier'" - technical reason, no human need
2. "Learning project that reaches 'because I wanted to learn React'" - valid motivation but not utility for others

**Gate 2 (Inversion) FAIL Examples:**
1. "CLI tool where users would just use the existing jq command with 2 extra flags" - trivial switching cost
2. "Dashboard that saves 5 minutes/month" - inconvenience too minor to matter

**Gate 3 (10-Minute) FAIL Examples:**
1. "Requires Docker, Redis, and API key signup before first use" - 30+ min to first value
2. "Demo works but no aha moment - just shows 'it installed'" - time OK but no demonstrated value

**Gate 4 (Bartender) FAIL Examples:**
1. "A distributed caching layer with eventual consistency guarantees" - pure jargon
2. "It helps developers by providing a CLI for their deployment pipeline" - jargon + implementation focus

Remove the "Status: Placeholder" line at the top. Keep the document header and structure.
  </action>
  <verify>
grep -c "### FAIL Example" skill/guides/GATE-EXAMPLES.md should return ≥8 (2+ per gate for Gates 1-4)
grep "Status: Placeholder" skill/guides/GATE-EXAMPLES.md should return nothing (placeholder removed)
  </verify>
  <done>Gates 1-4 each have 2+ documented FAIL examples with specific reasons tied to rubric criteria</done>
</task>

</tasks>

<verification>
After both tasks:
1. Gates 1-4 in gates-full.md have "Objective Rubric", "Scoring", "Edge Cases" sections
2. GATE-EXAMPLES.md has 2+ FAIL examples per gate for Gates 1-4
3. Rubric thresholds are numeric or binary (no subjective adjectives like "good", "clear", "reasonable")
4. FAIL examples reference specific rubric criteria in their explanations
</verification>

<success_criteria>
- Auditor can determine pass/fail for Gates 1-4 using explicit checklist or measurement
- No subjective interpretation required ("honest", "clear", "obvious" banned)
- FAIL examples define the floor - auditor knows what failure looks like
- Two auditors applying these rubrics to same project would reach same verdict
</success_criteria>

<output>
After completion, create `.planning/phases/02-gate-clarity/02-01-SUMMARY.md`
</output>
