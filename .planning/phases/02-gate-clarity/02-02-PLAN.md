---
phase: 02-gate-clarity
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - skill/config/gates-full.md
  - skill/guides/GATE-EXAMPLES.md
autonomous: true

must_haves:
  truths:
    - "Gates 5-8 have explicit pass/fail thresholds an auditor can apply without interpretation"
    - "Gate 5 (Wallet Test) uses multi-signal validation instead of 'honest yes'"
    - "Each of Gates 5-8 has 2+ FAIL examples showing clear failure patterns"
  artifacts:
    - path: "skill/config/gates-full.md"
      provides: "Objective rubrics for Gates 5-8 including market validation signals for Gate 5"
      contains: "multi-signal validation"
    - path: "skill/guides/GATE-EXAMPLES.md"
      provides: "FAIL examples for Gates 5-8"
      contains: "Gate 5"
  key_links:
    - from: "skill/config/gates-full.md"
      to: "skill/guides/GATE-EXAMPLES.md"
      via: "Gate rubrics reference examples"
      pattern: "See.*GATE-EXAMPLES.md"
---

<objective>
Add objective scoring rubrics and FAIL examples for Gates 5-8 (Wallet Test, Job-to-be-Done, Friction Audit, Second-Order Consequences). Special focus on Gate 5 which currently uses subjective "honest yes" language.

Purpose: Replace the most subjective gate criteria (especially Gate 5's "honest yes" and "real names") with multi-signal validation tests that produce consistent scoring.

Output: Updated gates-full.md with rubric sections for Gates 5-8, updated GATE-EXAMPLES.md with 2+ FAIL examples per gate.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-gate-clarity/02-RESEARCH.md

# Reference prior plan's changes
@.planning/phases/02-gate-clarity/02-01-SUMMARY.md

# Source files to modify
@skill/config/gates-full.md
@skill/guides/GATE-EXAMPLES.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add objective rubrics to Gates 5-8 in gates-full.md</name>
  <files>skill/config/gates-full.md</files>
  <action>
For each gate (5, 6, 7, 8), add three new sections after "Fail indicators" and before "Template Variables":

**Gate 5 (Wallet Test) - CRITICAL: Replace "honest yes" with multi-signal validation:**

```markdown
## Objective Rubric

**For self-audits (creator evaluating own project):**

Signal A: "I have paid for similar tools in this category in the past" [Yes/No]
Signal B: "I can name 3 specific people (real names, not archetypes) who have this exact problem" [Yes/No]
Signal C: "I have validated the problem through 5+ conversations with potential users" [Yes/No]
Signal D: "Comparable tools in this space charge $10-100/month" [Yes/No]

**Scoring:**
- Pass: 3+ signals = Yes
- Borderline: 2 signals = Yes → Apply counterfactual: "If you had to cancel one $20/month subscription to afford this, which would you cancel?" If you can answer specifically → PASS. If you hesitate → FAIL.
- Fail: 0-1 signals = Yes

**For third-party audits (auditor evaluating others' project):**

Signal A: GitHub stars >100 OR issues requesting features >10 [Yes/No]
Signal B: Comparable paid tools exist in same category [Yes/No]
Signal C: Project solves problem that costs time/money (not just convenience) [Yes/No]
Signal D: 3+ users report using in production (in Issues/PRs/Discussions) [Yes/No]

**Scoring:**
- Pass: 3+ signals = Yes
- Fail: 0-2 signals = Yes

**"Real names" validation:**
- PASS: Person you could message today who you've confirmed has this problem
- FAIL: Hypothetical persona ("a PM at a startup"), celebrity you've never talked to, person you assume has problem but haven't validated

## Edge Cases

- "Creator wouldn't pay because they built it" - see EDGE-CASES.md
- "Can name types but not names" - see EDGE-CASES.md
- "Open source projects where monetization isn't the goal" - see EDGE-CASES.md
```

**Gate 6 (Job-to-be-Done):**

```markdown
## Objective Rubric

Format check: "When [situation], I want to [motivation], so I can [outcome]"

**Checklist (all must be Yes for PASS):**
- [ ] Situation is specific (when/where/context, not just "when I need to")
- [ ] Motivation is action-oriented (verb phrase, not state)
- [ ] Outcome is tangible (measurable result or feeling)
- [ ] Single job (not multiple unrelated jobs combined)

**Specificity tests:**
- Situation: Could you observe someone in this situation? (Yes = specific)
- Motivation: Is the verb concrete? ("create" yes, "do" no, "manage" borderline)
- Outcome: Could you verify this outcome occurred? (Yes = tangible)

## Scoring

- Pass: All 4 checklist items = Yes
- Borderline: 3 of 4 = Yes → Which item failed? If situation vagueness, try adding context. If still can't specify → FAIL
- Fail: ≤2 of 4 = Yes

## Edge Cases

- "Multiple related jobs" - see EDGE-CASES.md
- "Generic situations" - see EDGE-CASES.md
```

**Gate 7 (Friction Audit):**

```markdown
## Objective Rubric

Score each step on friction level:
- Low (1): ≤2 minutes, no surprises
- Medium (2): 2-10 minutes OR minor confusion
- High (3): >10 minutes OR significant blocker

| Step | Measure | Low | Medium | High |
|------|---------|-----|--------|------|
| Discovery | Time to find from search | ≤1 min | 1-5 min | >5 min or not findable |
| Comprehension | Time to understand value | ≤1 min | 1-3 min | >3 min or still unclear |
| Installation | Time to get running | ≤3 min | 3-10 min | >10 min |
| First Use | Time to first value | ≤3 min | 3-10 min | >10 min |
| Repeated Use | Steps to repeat | ≤3 steps | 4-7 steps | >7 steps or manual each time |
| Mastery | Time to full proficiency | ≤30 min | 30-60 min | >60 min |

**Scoring:**
- Calculate friction score: Sum of all step scores (6-18 possible)
- Pass: Total ≤9 (average Low-Medium)
- Borderline: Total 10-12 → Check: Is highest friction step justified by value delivered? Yes = PASS, No = FAIL
- Fail: Total ≥13 OR any single step = High(3) without corresponding high value

## Edge Cases

- "Pre-requisites already installed" - see EDGE-CASES.md
- "Different friction for different user types" - see EDGE-CASES.md
```

**Gate 8 (Second-Order Consequences):**

```markdown
## Objective Rubric

Score each consequence:
- Positive: +1 per genuine positive second-order effect
- Negative: -1 per genuine negative second-order effect

**Positive indicators (each worth +1):**
- Would tell others (viral): Evidence of shareability (simple to explain, wow factor)
- Would build on it (ecosystem): API/plugin/extension points exist
- Would return (retention): Ongoing value, not one-time use

**Negative indicators (each worth -1):**
- Lock-in: Data/workflow harder to move than before adoption
- Maintenance burden: Requires ongoing attention beyond normal updates
- New dependency: Creates reliance that didn't exist before

**Scoring:**
- Calculate net: (Positive count) - (Negative count)
- Pass: Net ≥1 (more positive than negative)
- Borderline: Net = 0 → Check: Are positives high-impact and negatives low-impact? Yes = PASS, No = FAIL
- Fail: Net ≤-1 (more negative than positive)

## Edge Cases

- "Necessary lock-in" (e.g., data format standards) - see EDGE-CASES.md
- "Maintenance as feature" (e.g., security updates) - see EDGE-CASES.md
```
  </action>
  <verify>
grep -c "## Objective Rubric" skill/config/gates-full.md should return 8 (Gates 1-8 now have rubrics)
grep "honest yes" skill/config/gates-full.md should return nothing (subjective language removed)
grep -c "multi-signal validation\|Signal A:" skill/config/gates-full.md should return ≥1 (Gate 5 has signals)
  </verify>
  <done>Gates 5-8 each have Objective Rubric, Scoring, and Edge Cases sections; Gate 5 "honest yes" replaced with multi-signal validation</done>
</task>

<task type="auto">
  <name>Task 2: Add FAIL examples for Gates 5-8 to GATE-EXAMPLES.md</name>
  <files>skill/guides/GATE-EXAMPLES.md</files>
  <action>
Add FAIL examples for Gates 5-8, following the same structure established in 02-01 for Gates 1-4.

**Gate 5 (Wallet Test) FAIL Examples:**

1. "Can only name archetypes, not real people"
   - Project: Task management CLI
   - Attempted: "Developers would pay for this" / "Project managers need this"
   - Why FAILS: Archetypes (developers, PMs) not real names. Cannot message these people today.
   - Contrast: "John Smith (my coworker who complained about this last week)" = PASS

2. "Self-audit: Wouldn't pay because already built it"
   - Project: Personal automation script
   - Attempted: "I wouldn't pay because I already have it"
   - Why FAILS: Revealed preference shows value is below $20/month. Reframe test failed.
   - Contrast: "If I hadn't built this, I would absolutely pay $20/month - I've paid for similar tools like X, Y, Z" = PASS

3. "Third-party audit: No market validation signals"
   - Project: GitHub repo with 12 stars, no issues, no production users
   - Attempted: Claims "solves real problem" but no evidence
   - Why FAILS: 0 of 4 third-party signals met (stars <100, no feature requests, no production usage reported)
   - Contrast: 150 stars + 15 feature request issues + 3 users reporting production use = PASS

**Gate 6 (JTBD) FAIL Examples:**

1. "Vague situation"
   - Attempted: "When I need to be more productive..."
   - Why FAILS: "Need to be productive" is not observable. When/where/context missing.
   - Contrast: "When I'm reviewing my weekly tasks on Monday morning..." = specific, observable

2. "Multiple unrelated jobs"
   - Attempted: "When I need to track time AND manage invoices AND schedule meetings..."
   - Why FAILS: Three separate jobs crammed together. Each deserves its own JTBD.
   - Contrast: Pick one primary job; others are supporting features

**Gate 7 (Friction Audit) FAIL Examples:**

1. "High friction installation"
   - Project: Requires Docker, Redis, PostgreSQL, and 3 API key signups
   - Friction scores: Discovery=1, Comprehension=2, Installation=3, First Use=3, Repeated=2, Mastery=2
   - Total: 13 (threshold is ≤9 for pass)
   - Why FAILS: Installation alone is >10 minutes with multiple blocking steps

2. "Hidden friction in repeated use"
   - Project: Works great first time, but requires manual config each subsequent use
   - Friction scores: Discovery=1, Comprehension=1, Installation=2, First Use=2, Repeated=3, Mastery=3
   - Total: 12 (borderline, but repeated use High friction without high value)
   - Why FAILS: Users won't adopt if repeated use is painful

**Gate 8 (Second-Order) FAIL Examples:**

1. "Creates worse lock-in than problem it solves"
   - Project: Data migration tool that uses proprietary intermediate format
   - Positives: Retention (+1)
   - Negatives: Lock-in (-1), New dependency (-1)
   - Net: -1
   - Why FAILS: Solving migration by creating new migration problem

2. "One-time use, no retention"
   - Project: Code formatter that runs once on legacy codebase
   - Positives: None ongoing
   - Negatives: None significant
   - Net: 0, but no high-impact positives
   - Why FAILS: Borderline test - no ongoing value, no ecosystem, no viral potential
  </action>
  <verify>
grep -c "### FAIL Example" skill/guides/GATE-EXAMPLES.md should return ≥16 (2+ per gate for Gates 1-8)
grep -c "Gate 5" skill/guides/GATE-EXAMPLES.md should return ≥3 (header + 2 examples minimum)
  </verify>
  <done>Gates 5-8 each have 2+ documented FAIL examples with specific reasons tied to rubric criteria</done>
</task>

</tasks>

<verification>
After both tasks:
1. Gates 5-8 in gates-full.md have "Objective Rubric", "Scoring", "Edge Cases" sections
2. Gate 5 no longer contains "honest yes" - replaced with multi-signal validation
3. GATE-EXAMPLES.md has 2+ FAIL examples per gate for Gates 5-8
4. Gate 5 examples specifically show persona vs. real name distinction
</verification>

<success_criteria>
- Gate 5 "honest yes" is replaced with countable signals (3+ of 4 = pass)
- Gate 5 "real names" has explicit validation criteria (can message today + confirmed problem)
- Gates 6-8 have measurable thresholds (checklists, friction scores, net positive count)
- FAIL examples for Gate 5 show common failure patterns (archetypes, reframe failure, no market signals)
</success_criteria>

<output>
After completion, create `.planning/phases/02-gate-clarity/02-02-SUMMARY.md`
</output>
