---
phase: 02-gate-clarity
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - skill/config/gates-full.md
  - skill/guides/GATE-EXAMPLES.md
autonomous: true

must_haves:
  truths:
    - "Gates 9-11 have explicit pass/fail thresholds an auditor can apply without interpretation"
    - "Gate 10 (Pareto) uses concentration ratio calculation instead of 'core value concentrated'"
    - "Gate 11 (Regret) uses commitment signals instead of 'would regret NOT shipping'"
    - "Each of Gates 9-11 has 2+ FAIL examples showing clear failure patterns"
  artifacts:
    - path: "skill/config/gates-full.md"
      provides: "Objective rubrics for all 11 gates including Pareto calculation and commitment signals"
      contains: "concentration ratio"
    - path: "skill/guides/GATE-EXAMPLES.md"
      provides: "FAIL examples for all 11 gates"
      contains: "Gate 11"
  key_links:
    - from: "skill/config/gates-full.md"
      to: "skill/guides/GATE-EXAMPLES.md"
      via: "Gate rubrics reference examples"
      pattern: "See.*GATE-EXAMPLES.md"
---

<objective>
Add objective scoring rubrics and FAIL examples for Gates 9-11 (Occam's Razor, Pareto Gate, Regret Minimization). Special focus on Gates 10 and 11 which have the most subjective criteria in the current framework.

Purpose: Transform the most subjective gates (Pareto's "concentrated value" and Regret's "would regret") into measurable criteria using research-backed patterns: concentration ratio for Pareto, commitment signals for Regret.

Output: Updated gates-full.md with rubric sections for Gates 9-11, updated GATE-EXAMPLES.md with 2+ FAIL examples per gate. All 11 gates now have objective rubrics.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-gate-clarity/02-RESEARCH.md

# Reference prior plans
@.planning/phases/02-gate-clarity/02-01-SUMMARY.md
@.planning/phases/02-gate-clarity/02-02-SUMMARY.md

# Source files to modify
@skill/config/gates-full.md
@skill/guides/GATE-EXAMPLES.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add objective rubrics to Gates 9-11 in gates-full.md</name>
  <files>skill/config/gates-full.md</files>
  <action>
For each gate (9, 10, 11), add three new sections after "Fail indicators" and before "Template Variables":

**Gate 9 (Occam's Razor):**

```markdown
## Objective Rubric

**Simplicity test checklist:**

1. **Feature count test:** Could you remove features and still solve the core problem?
   - List all features
   - For each: "Is this essential to the core job-to-be-done?" [Yes/No]
   - If >30% of features are "No" → Potential over-engineering

2. **Alternative existence test:** Does a simpler solution exist?
   - Can a bash script/one-liner solve 80%+ of this? [Yes/No]
   - Does an existing tool solve this with acceptable trade-offs? [Yes/No]
   - If either = Yes AND you can't articulate why your solution is necessary → FAIL

3. **Abstraction level test:** Are you building a framework when a function would do?
   - Count layers of abstraction (config files, plugins, extension points)
   - Is each layer justified by actual use cases? [Yes/No for each]
   - If any layer lacks use case justification → Over-engineered

**Scoring:**
- Pass: All tests pass (essential features only, no simpler alternative, justified abstractions)
- Borderline: 1 test fails → Is the complexity justified by scope or future needs? Document rationale.
- Fail: 2+ tests fail OR "just in case" features exist

## Edge Cases

- "Necessary complexity" (security, compliance) - see EDGE-CASES.md
- "Platform vs. tool" distinction - see EDGE-CASES.md
```

**Gate 10 (Pareto Gate) - CRITICAL: Replace "concentrated value" with calculation:**

```markdown
## Objective Rubric

**Value Concentration Calculation:**

1. List all user-facing features (N = total features)
   - Feature = distinct capability user can invoke independently
   - Variations/parameters of same capability = 1 feature
   - Example: "Search with filters" = 1 feature, not 5 features

2. Score each feature on impact (1-10 scale):
   - 10: Core functionality, project useless without it
   - 7-9: Major value driver, significant user benefit
   - 4-6: Meaningful enhancement, clear use case
   - 1-3: Edge case, convenience, or "nice to have"

3. Sort features by impact score (descending)

4. Calculate cumulative impact percentage:
   - Sum all impact scores = Total Impact
   - For each feature (top to bottom): Cumulative% = (Sum so far / Total Impact) × 100

5. Find K = number of features needed to reach 80% cumulative impact

6. Calculate concentration ratio: K/N

**Scoring:**
- Pass: K/N ≤ 0.30 (top 30% of features deliver 80% of value)
- Borderline: 0.30 < K/N ≤ 0.50 → Apply binary test below
- Fail: K/N > 0.50 (value spread too thin)

**Binary alternative (if scoring feels subjective):**
- Can you name the ONE feature that delivers majority of value? [Yes/No]
- If that feature was removed, would project still be useful? [Yes/No]
- Are other features primarily enhancements to the core feature? [Yes/No]

Pass: Q1=Yes AND (Q2=No OR Q3=Yes)
Fail: Q1=No (can't identify core feature)

## Documentation Required

- Feature list with impact scores
- Concentration ratio calculation
- If borderline: rationale for binary test result

## Edge Cases

- "Feature count ambiguity" - see EDGE-CASES.md
- "All features seem essential" - see EDGE-CASES.md
- "Platform coherence" - see EDGE-CASES.md
```

**Gate 11 (Regret Minimization) - CRITICAL: Replace "would regret" with commitment signals:**

```markdown
## Objective Rubric

**For self-audits (creator evaluating own project):**

Commitment signals (need 5 of 6 for PASS):
1. "I have been manually doing this task for 6+ months" [Yes/No]
2. "I searched for existing solutions before building" [Yes/No]
3. "Existing solutions were inadequate (can list specific reasons)" [Yes/No]
4. "I will use this weekly (or monthly if high-impact) after building" [Yes/No]
5. "This problem costs me 2+ hours per week (or 5+ hours per month)" [Yes/No]
6. "I would rebuild this if the repo was deleted" [Yes/No]

**Scoring:**
- Pass: 5+ signals = Yes
- Borderline: 4 signals = Yes → Apply counterfactual: "Would you recommend a friend spend 40 hours building this?" If unhesitating yes → PASS. If hesitation → FAIL.
- Fail: 0-3 signals = Yes

**For third-party audits (auditor evaluating others' project):**

Commitment signals (need 4 of 5 for PASS):
1. Commit history spans 3+ months (not one-time build) [Yes/No]
2. Issues and PRs show active response (median response time <7 days) [Yes/No]
3. Documentation includes roadmap or future plans [Yes/No]
4. Project solves problem creator publicly described before building [Yes/No]
5. Creator has built multiple projects in this domain (not one-off) [Yes/No]

**Scoring:**
- Pass: 4+ signals = Yes
- Fail: 0-3 signals = Yes

**Automatic FAIL disqualifiers:**
- README explicitly states "learning exercise" or "tutorial project"
- Project is fork/clone of existing tool with no meaningful differentiation
- Creator states in issues/docs they don't use the tool themselves

## Edge Cases

- "Long-standing problem, new solution approach" - see EDGE-CASES.md
- "High initial commitment, low future use" - see EDGE-CASES.md
- "Market timing vs. commitment" - see EDGE-CASES.md
```
  </action>
  <verify>
grep -c "## Objective Rubric" skill/config/gates-full.md should return 11 (all gates now have rubrics)
grep "concentrated value\|would regret" skill/config/gates-full.md should return 0 in rubric sections (replaced with specific criteria)
grep -c "concentration ratio\|K/N" skill/config/gates-full.md should return ≥2 (Gate 10 has calculation)
grep -c "commitment signals\|Signal" skill/config/gates-full.md should return ≥2 (Gate 11 has signals)
  </verify>
  <done>Gates 9-11 each have Objective Rubric, Scoring, and Edge Cases sections; Gate 10 has concentration ratio, Gate 11 has commitment signals</done>
</task>

<task type="auto">
  <name>Task 2: Add FAIL examples for Gates 9-11 to GATE-EXAMPLES.md</name>
  <files>skill/guides/GATE-EXAMPLES.md</files>
  <action>
Add FAIL examples for Gates 9-11, following the structure established in previous plans.

**Gate 9 (Occam's Razor) FAIL Examples:**

1. "Framework when function would do"
   - Project: Plugin system with config files, extension points, lifecycle hooks
   - Use case: One internal tool that needs to format JSON
   - Why FAILS: Abstraction test fails - 3 layers of abstraction with no justified use cases
   - Contrast: Simple function that formats JSON, maybe with 1-2 options = PASS

2. "Bash script would solve this"
   - Project: 500-line Node.js CLI tool
   - Core functionality: Renames files matching a pattern
   - Why FAILS: `find . -name "*.txt" -exec mv {} {}.bak \;` does 80%+
   - Contrast: If the tool adds significant value over bash (error handling, dry-run, undo) = PASS

**Gate 10 (Pareto Gate) FAIL Examples:**

1. "Can't identify the ONE feature"
   - Project: "All-in-one productivity suite"
   - Feature list: 15 features, creator says "all equally important"
   - Concentration analysis: K/N = 0.67 (10 of 15 features needed for 80% value)
   - Why FAILS: Value spread too thin, no core identity
   - Contrast: "It's a note-taking app. Search and sync are nice-to-haves." = clear core

2. "Feature creep without usage data"
   - Project: Started as simple file watcher, now has 12 features
   - Impact scores: Core watch = 9, notification = 7, then 10 features scored 3-5
   - K/N = 0.50 (6 features for 80%)
   - Why FAILS: Borderline, and binary test reveals core feature unclear
   - Contrast: Would pass if features 3-12 were extensions of core watching capability

**Gate 11 (Regret Minimization) FAIL Examples:**

1. "Resume-driven development"
   - Self-audit signals check:
     - 6+ months manual work: No (just heard about this tech)
     - Searched for alternatives: No (wanted to learn the new framework)
     - Will use weekly: Maybe
     - Costs 2+ hours/week: No
     - Would rebuild: No
   - Score: 1/6 signals
   - Why FAILS: Building to add to resume, not to solve real problem

2. "Learning project presented as production tool"
   - Third-party audit signals check:
     - 3+ month commit history: No (all commits in 2-week burst)
     - Active issue response: N/A (no issues)
     - Roadmap exists: No
     - Creator described problem before: No
     - Multiple domain projects: No
   - Score: 0/5 signals
   - Disqualifier: README doesn't say "learning" but commit messages reference tutorial
   - Why FAILS: No evidence of sustained commitment or real problem validation

3. "High initial commitment, abandoned"
   - Third-party audit signals check:
     - 3+ month history: Yes (but last commit 8 months ago)
     - Active response: No (5 open issues, none responded)
     - Roadmap: Yes (but all items stale)
     - Problem described before: Yes
     - Multiple projects: Yes
   - Score: 3/5 signals (borderline if only counting positive signals)
   - Why FAILS: Commitment evidence is historical, not current. Abandon pattern suggests regret already happened.
   - Contrast: Same project with recent activity, responded issues = PASS
  </action>
  <verify>
grep -c "### FAIL Example" skill/guides/GATE-EXAMPLES.md should return ≥22 (2+ per gate for all 11 gates)
grep -c "Gate 10\|Gate 11" skill/guides/GATE-EXAMPLES.md should return ≥6 (headers + examples for both gates)
  </verify>
  <done>Gates 9-11 each have 2+ documented FAIL examples; all 11 gates now have FAIL examples with specific reasons tied to rubric criteria</done>
</task>

</tasks>

<verification>
After both tasks:
1. All 11 gates in gates-full.md have "Objective Rubric", "Scoring", "Edge Cases" sections
2. Gate 10 uses concentration ratio (K/N ≤ 0.30 for pass)
3. Gate 11 uses commitment signals (5/6 for self-audit, 4/5 for third-party)
4. GATE-EXAMPLES.md has 2+ FAIL examples per gate for all 11 gates
5. No subjective language remains in rubrics ("honest", "clear", "obvious", "concentrated", "would regret")
</verification>

<success_criteria>
- Gate 10 "core value concentrated" is replaced with measurable concentration ratio
- Gate 11 "would regret NOT shipping" is replaced with observable commitment signals
- All 11 gates have explicit pass/fail thresholds
- Total of 22+ FAIL examples across all gates (2+ per gate minimum)
- Two auditors applying these rubrics to same project would reach same verdict
</success_criteria>

<output>
After completion, create `.planning/phases/02-gate-clarity/02-03-SUMMARY.md`
</output>
